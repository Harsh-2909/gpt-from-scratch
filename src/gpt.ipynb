{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW3sKbAcjDrLu5cpnZkq2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harsh-2909/GenAI-Learning/blob/main/gpt/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16FnjeIxTCnl",
        "outputId": "d4a5d550-d03e-4fc6-9f4a-8da413264244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-18 06:18:37--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-12-18 06:18:37 (18.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Lets get the data first\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "GPwvEK5bTVfh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)\n",
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "tuhgmTK5Tohe",
        "outputId": "afc619b0-e402-40b6-9999-3032bbb959c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the vocabulary of the training data\n",
        "char = sorted(list(set(text)))\n",
        "print(''.join(char))\n",
        "print(len(char))\n",
        "vocab_size = len(char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YNCjxcJTv21",
        "outputId": "06a9dcad-c4af-4336-abc8-a93150ebbc04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we create a tokenizer to tokenize the inputs\n",
        "# A tokenizer simply converts the inputs into a list of integers where each integer represent a token (which can be a word, sub-word, or character)\n",
        "# There are multiple types of tokenizers like word, sub-word, and character based tokenizer.\n",
        "# Here, we will be using a character tokenizer.\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(char)}\n",
        "itos = {i: ch for i, ch in enumerate(char)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "s = \"\"\"Hey Jude\n",
        "Don't make it bad\"\"\"\n",
        "print(encode(s))\n",
        "print(decode(encode(s)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dk4gUZ1Uz8f",
        "outputId": "57f7da4c-355c-4b76-a829-b6371a5181c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 63, 1, 22, 59, 42, 43, 0, 16, 53, 52, 5, 58, 1, 51, 39, 49, 43, 1, 47, 58, 1, 40, 39, 42]\n",
            "Hey Jude\n",
            "Don't make it bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we encode the entire text and convert it into tensors using PyTorch\n",
        "import torch\n",
        "encoded_text = encode(text)\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlPzmMTya8Co",
        "outputId": "007300ac-e26e-4ace-8974-a1893ec2b2e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
              "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
              "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
              "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
              "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
              "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
              "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
              "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
              "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
              "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
              "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
              "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
              "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
              "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
              "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
              "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
              "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
              "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
              "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
              "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
              "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
              "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
              "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
              "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
              "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
              "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
              "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
              "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
              "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
              "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
              "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
              "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
              "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
              "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
              "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
              "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
              "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
              "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
              "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
              "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
              "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
              "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
              "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
              "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
              "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
              "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
              "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
              "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
              "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and validation chunks\n",
        "train_data_percent = 0.9 # 90% would be for training\n",
        "n = int(train_data_percent * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "deVu6bqacGEF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot train a model with all the training data at once, as it is very expensive computationally. Thus, we train the model chunk by chunk where each chunk is taken out randomly from the data. Here, the chunk size will be indicated by the variable `block_size`.\n"
      ],
      "metadata": {
        "id": "gdvn9qxc0kor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "# train_data[:block_size+1]\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtO1fiu41lf-",
        "outputId": "fc1f66e8-b61e-4f09-f4c1-b3cd5b522016"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The training chunk of block size gives example to the model on how to respond when facing a set of inputs. For example, here when [18] is sent as input then return 47.\n",
        "  Similarly, when [18, 47] is sent as input, then return 56. Thus, this gives us `block_size` number of examples from context length of 1 to block_size.\n",
        "\n",
        "  Here, we will also need to add batch_size to do parallel processing to make training more efficient. Each block in a batch is processed parallelly without any interference."
      ],
      "metadata": {
        "id": "hdllAkab5tH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) # Manually seeding to get deterministic random numbers\n",
        "batch_size = 4 # The number of parallel independent sequences\n",
        "block_size = 8 # The number of tokens processed at a time. Maximum context length of predictions\n",
        "\n",
        "def get_batch(split_type):\n",
        "  \"\"\"Generates a small batch of data of inputs x and target y\"\"\"\n",
        "  data = train_data if split_type == \"train\" else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size, )) # generates a list of offsets randomly of size `batch_size`\n",
        "  x = torch.stack([data[i: i+block_size] for i in ix]) # Here, we are creating the context blocks\n",
        "  y = torch.stack([data[i+1: i+block_size+1] for i in ix]) # Target Blocks. This should be the output when a context block is passed to the model\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"inputs:\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"targets:\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target is {target}\")\n",
        "    # Here, this gives us 32 different context (x) with their required targets (y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-u6fyur6ToA",
        "outputId": "3858bee4-19c6-4c62-ce11-0ba4d76508e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target is 43\n",
            "when input is [24, 43] the target is 58\n",
            "when input is [24, 43, 58] the target is 5\n",
            "when input is [24, 43, 58, 5] the target is 57\n",
            "when input is [24, 43, 58, 5, 57] the target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
            "when input is [44] the target is 53\n",
            "when input is [44, 53] the target is 56\n",
            "when input is [44, 53, 56] the target is 1\n",
            "when input is [44, 53, 56, 1] the target is 58\n",
            "when input is [44, 53, 56, 1, 58] the target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52] the target is 58\n",
            "when input is [52, 58] the target is 1\n",
            "when input is [52, 58, 1] the target is 58\n",
            "when input is [52, 58, 1, 58] the target is 46\n",
            "when input is [52, 58, 1, 58, 46] the target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
            "when input is [25] the target is 17\n",
            "when input is [25, 17] the target is 27\n",
            "when input is [25, 17, 27] the target is 10\n",
            "when input is [25, 17, 27, 10] the target is 0\n",
            "when input is [25, 17, 27, 10, 0] the target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  \"\"\"We are using the Bigram language model. It is a part of n-gram models. A bigram model uses the context of the previous 1 token to predict the next token. It does not check the context beyond 1 token.\"\"\"\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits from the lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    logits = self.token_embedding_table(idx) # Returns a tensor of dim (batch_size, block_size, vocab_size) (B, T, C). Here its, [4, 8, 65]\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      # Converting 3D array to 2D array where B and T dimensions are converted to a single dimension while preserving the C dimension.\n",
        "      # This is done to conform with the cross_entropy documentation.\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets) # loss should be -ln(1/vocab_size). Check Karpathy videos for more understanding.\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"idx is (B, T) array. This function will take the input tokens `idx` and generate the output tokens based on the inputs.\n",
        "\n",
        "    Arguments:\n",
        "        idx {Array} -- An array of size (B, T)\n",
        "        max_new_tokens {Integer} -- The number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        [Array] -- The output of size (B, T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # First get the prediction using the forward function.\n",
        "      logits, loss = self(idx)\n",
        "      # Then only keep the last index of the T dimension because each of the element in T represents a token and we are only focused om the last token for each prediction.\n",
        "      logits = logits[:, -1, :] # Shape: (B, C)\n",
        "      # Apply softmax to get the probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # Shape: (B, C)\n",
        "      # Get the sample for the distribution.\n",
        "      # After the softmax, we are getting the a 2D array where each row element (which denotes the batch) has an array of length C.\n",
        "      # That array is the probability distribution of what would be the next token for each batch.\n",
        "      # Now, the multinomial function samples the probability distribution and returns the single token from the probability distribution.\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1) # Shape: (B, 1)\n",
        "      # Append the prediction to the input sequence.\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # Shape: (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "logits, loss = model(xb, yb)\n",
        "# logits, loss = model(xb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long) # Generates token 0 which corresponds to newline. This will be used to start generation\n",
        "generated_tokens = model.generate(idx, max_new_tokens = 100)[0]\n",
        "output = decode(generated_tokens.tolist())\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0A7XnQKKucP",
        "outputId": "92d1a99d-0e90-4f24-bb0d-40ea24f411fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output we got was completely garbage values because the embedding table was randomly generated without any training. Lets train the model first before using it."
      ],
      "metadata": {
        "id": "3alIMLOmU07z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3) # 3r-4 is a good learning rate for Bigger models.\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K9i0a-FU_bm",
        "outputId": "099a5a91-566f-44b1-ad09-5ef54e6b2eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5727508068084717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long) # Generates token 0 which corresponds to newline. This will be used to start generation\n",
        "generated_tokens = model.generate(idx, max_new_tokens = 500)[0]\n",
        "output = decode(generated_tokens.tolist())\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK-HwE_XXcj4",
        "outputId": "54b64003-3d29-49c0-f826-95e5f0226d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht anjx?\n",
            "\n",
            "DUThinqunt.\n",
            "\n",
            "LaZAnde.\n",
            "athave l.\n",
            "KEONH:\n",
            "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
            "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
            "I hisgothers je are!-e!\n",
            "QLYotouciullle'z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The mathematical trick in Self Attention"
      ],
      "metadata": {
        "id": "6xPebZGWHpOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMMUvlS7HlEG",
        "outputId": "3cdbbbdd-b55d-4dc5-94dc-eee684b43ef6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1: Averaging past context with for loop, the weakest form of aggregation.\n",
        "\n",
        "# We want x[b, t] = mean_{i<=t} x[b, i]\n",
        "# A token needs the context of the prev tokens (decoder architecture) so that we can understand the context of the token with respect to the sentence and predict the next tokens.\n",
        "# Currently, we are averaging out the context of all the prev tokens (from 0 to t token) to get the new context.\n",
        "x_bag_of_words = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    x_prev = x[b, :t+1] # Dim: (t, C) because we are calculating x_prev for each batch, so there is no batch dim for this.\n",
        "    x_bag_of_words[b, t] = torch.mean(x_prev, 0) # Dim: (C) We are avg out the t dimension to get the mean of the context (C) dimension\n",
        "\n",
        "print(x[1])\n",
        "print(x_bag_of_words[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ViQOtrIKyu",
        "outputId": "3bfa5951-3bd9-46b5-9582-5f0498340cb8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348],\n",
            "        [ 0.4478, -0.8016],\n",
            "        [ 1.5236,  2.5086]])\n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.8173,  0.4127],\n",
            "        [-0.1342,  0.4395],\n",
            "        [ 0.2711,  0.4774],\n",
            "        [ 0.2421,  0.0694],\n",
            "        [ 0.0084,  0.0020],\n",
            "        [ 0.0712, -0.1128],\n",
            "        [ 0.2527,  0.2149]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix multiply as weighted aggregation\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.ones(3, 3)\n",
        "a = torch.tril(a)\n",
        "a = a / torch.sum(a, 1, keepdim=True) # With this, the 1st row will only have the context from 1st row, the 2nd row will be the avg of 1st and 2nd row, and so on.\n",
        "# This is exactly what we did above to find the mean of the context dim but with pure maths, improving performance.\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a @ b\n",
        "\n",
        "print(f\"a = {a}\")\n",
        "print(f\"b = {b}\")\n",
        "print(f\"c = {c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0UurkYdL9nh",
        "outputId": "3c0f31bb-0d55-45e2-b500-1239e18419a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b = tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c = tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2: Self Attention using Matrix Multiplication\n",
        "weights = torch.tril(torch.ones(T, T))\n",
        "weights = weights / weights.sum(1, keepdim=True)\n",
        "\n",
        "x_bag_of_words2 = weights @ x # (B, T, T) @ (B, T, C) -> (B, T, C) [PyTorch will automatically add B dimension to the weights and do parallel multiplication]\n",
        "torch.allclose(x_bag_of_words, x_bag_of_words2) # This will come out as True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g66ViCNrQTk7",
        "outputId": "66e17b68-c7e4-424b-90ef-349ca0d33e06"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3: Using softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "weights = torch.zeros((T, T))\n",
        "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "x_bow3 = weights @ x\n",
        "torch.allclose(x_bag_of_words, x_bow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCzGGCq9Vtia",
        "outputId": "dd5e0d68-7bd3-4e8d-a445-e1d61bbcc10f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention\n",
        "\n",
        "Here comes the most important part of transformers, self-attention.\n",
        "In the prev versions, we were getting the weighted aggregation of all the tokens in equal weight. It didnot matter what is the position or if we need this token or not.\n",
        "\n",
        "For a better way, we need the tokens to interact with other tokens where those tokens have similarity.\n",
        "A token generates 2 layers, a query and a key.\n",
        "A query is what the token needs while the key is what the token is.\n",
        "To find the best corelation, we multiply the query with the keys of other tokens to get the new weights."
      ],
      "metadata": {
        "id": "WE6S4W8syLHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: Self Attention\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Lets see a single head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "# This step happens in parallel for all the tokens\n",
        "k = key(x)   # (B, T, head_size=16)\n",
        "q = query(x) # (B, T, head_size=16)\n",
        "weights = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# weights = torch.zeros((T, T))\n",
        "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = weights @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nReoHMQdv91R",
        "outputId": "f1ec2103-bc6c-439a-d412-7585583e97c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that points to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. That's why, we need to positionally encode tokens.\n",
        "- Each example across batch dimension is processed completely independently and the nodes from one batch never \"talk\" to the nodes of other batch. They also process in parallel.\n",
        "- In an \"encoder\" attention block just deletes the line which does masking of weights using `tril`, allowing all the tokens to communicate. Eg: Sentiment analysis, data parsing, or doing text to image generation.\n",
        "Here, it is a \"decoder\" attention block because it has triangular masking, and it is used in autoregressive settings, like when generating text.\n",
        "\n",
        "Difference between self-attention & cross-attention:\n",
        "\n",
        "- This attention we just made is only called `self-attention` because the `query`, `key` and `value` all comes from the same source (x in this case). That means all the tokens in a batch will only talk to each other for the context and then do the weighted aggregation.\n",
        "- Instead, in `cross-attention`, the `query` is generated from one source (would be x in this case), while the `key` and `value` will be generated from a different data source.\n",
        "It can be embeddings from a vector store, or just any context we have saved previously in multi-shot learning. It can also be the data we have already processed from the encoder in an encoder-decoder architecture (like summarization, translation, etc)."
      ],
      "metadata": {
        "id": "Nm5KVeGHckCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why to multiply the weights with `1/sqrt(head_size)` before `softmax`?\n",
        "\n",
        "\"Scaled\" attention additional divides `weights` by `1/sqrt(head_size)`. This makes it so when input Q, K are unit variance, weight will be unit variance too and Softmax will stay diffused and not saturated too much."
      ],
      "metadata": {
        "id": "rUjcq-QroiI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "q = torch.randn(B, T, head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "2UV0Bn733iaH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEE2GqjPrily",
        "outputId": "00c060cc-12c0-43d1-be57-eb50cb02cff8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0966)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVFSvPvoroHS",
        "outputId": "8b6463d8-bfde-40dd-ceec-1a5d742285ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9416)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that without the square root, the wei variance comes close to head_size.\n",
        "# With the square root, the variance converges to 1.\n",
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCTT5ijBrpQf",
        "outputId": "cadc66b2-3fc8-40cd-ee79-93e92468647a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0065)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During initialization, we need the weights to be diffused.\n",
        "In the example below, when we generated softmax of fairly close values, then the softmax was fairly diffused.\n",
        "But when we increased the extremes of the values before softmax, the softmax result started focusing on just the highest value which is a single vector instead of multiple vectors. If this continues, then the tokens will only communicate with a single token every time instead of all the tokens."
      ],
      "metadata": {
        "id": "873kGGEfsbW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vt04mMLr_Uw",
        "outputId": "bc7d099c-36b1-4d2e-c0ad-8a9389c7fe99"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bibw1__KsIxm",
        "outputId": "e2bff2dc-fdeb-48c8-a10e-a7e0f5df1cca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}